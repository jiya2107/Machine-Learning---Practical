{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxZRqACjf5u/Yx5pxC441L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiya2107/Machine-Learning---Practical/blob/main/ML_Experiment_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-IC9LegtkcB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Activation and Loss Functions ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    # If x is the input (pre-activation), calculate sigmoid(x) * (1 - sigmoid(x))\n",
        "    # If x is already the sigmoid output (a), use a * (1 - a)\n",
        "    return x * (1 - x)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# --- 2. Initial Setup (Simplified 2-Layer Network) ---\n",
        "np.random.seed(1)\n",
        "# 2 inputs, 2 hidden neurons, 1 output neuron\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Initial weights and biases\n",
        "W1 = np.random.uniform(low=-0.5, high=0.5, size=(input_size, hidden_size)) # (2, 2)\n",
        "B1 = np.zeros((1, hidden_size)) # (1, 2)\n",
        "W2 = np.random.uniform(low=-0.5, high=0.5, size=(hidden_size, output_size)) # (2, 1)\n",
        "B2 = np.zeros((1, output_size)) # (1, 1)\n",
        "\n",
        "# Sample Data (e.g., XOR example)\n",
        "X = np.array([[0, 1]]) # Single input sample\n",
        "y_true = np.array([[1]]) # Target output\n",
        "\n",
        "print(\"--- Multi-layer Perceptron (MLP) & Backpropagation Demonstration ---\")\n",
        "print(f\"Initial W1:\\n{W1.round(4)}\")\n",
        "print(f\"Initial W2:\\n{W2.round(4)}\")\n",
        "\n",
        "# --- 3. FORWARD PASS ---\n",
        "print(\"\\n[STEP 1: FORWARD PASS]\")\n",
        "# Hidden Layer\n",
        "Z1 = np.dot(X, W1) + B1 # Pre-activation\n",
        "A1 = sigmoid(Z1) # Activation (Output of Hidden Layer)\n",
        "print(f\"Hidden Layer Output (A1): {A1.round(4)}\")\n",
        "\n",
        "# Output Layer\n",
        "Z2 = np.dot(A1, W2) + B2 # Pre-activation\n",
        "y_pred = sigmoid(Z2) # Final Prediction\n",
        "print(f\"Final Prediction (y_pred): {y_pred.round(4)}\")\n",
        "loss = mse_loss(y_true, y_pred)\n",
        "print(f\"Initial MSE Loss: {loss.round(4)}\")\n",
        "\n",
        "# --- 4. BACKWARD PASS (The Core of Backpropagation) ---\n",
        "print(\"\\n[STEP 2: BACKWARD PASS (Gradient Calculation)]\")\n",
        "\n",
        "# A. Output Layer Error (dLoss/dZ2)\n",
        "# dLoss/dy_pred * dy_pred/dZ2\n",
        "# dLoss/dy_pred = -(y_true - y_pred) for MSE\n",
        "# dy_pred/dZ2 = sigmoid_derivative(y_pred)\n",
        "dLoss_dZ2 = (y_pred - y_true) * sigmoid_derivative(y_pred) # (1, 1)\n",
        "print(f\"dLoss/dZ2: {dLoss_dZ2.round(4)}\")\n",
        "\n",
        "# B. Output Layer Weights Gradient (dLoss/dW2)\n",
        "# dLoss/dW2 = A1.T * dLoss/dZ2\n",
        "dLoss_dW2 = np.dot(A1.T, dLoss_dZ2) # (2, 1)\n",
        "print(f\"dLoss/dW2:\\n{dLoss_dW2.round(4)}\")\n",
        "\n",
        "# C. Hidden Layer Error (dLoss/dZ1)\n",
        "# dLoss/dZ1 = (dLoss/dZ2 * W2.T) * dZ1/dA1\n",
        "dLoss_dA1 = np.dot(dLoss_dZ2, W2.T) # (1, 2)\n",
        "dLoss_dZ1 = dLoss_dA1 * sigmoid_derivative(A1) # (1, 2)\n",
        "print(f\"dLoss/dZ1: {dLoss_dZ1.round(4)}\")\n",
        "\n",
        "# D. Hidden Layer Weights Gradient (dLoss/dW1)\n",
        "# dLoss/dW1 = X.T * dLoss/dZ1\n",
        "dLoss_dW1 = np.dot(X.T, dLoss_dZ1) # (2, 2)\n",
        "print(f\"dLoss/dW1:\\n{dLoss_dW1.round(4)}\")\n",
        "\n",
        "# --- 5. WEIGHT UPDATE (Gradient Descent) ---\n",
        "print(\"\\n[STEP 3: WEIGHT UPDATE]\")\n",
        "learning_rate = 0.1\n",
        "\n",
        "W2_new = W2 - learning_rate * dLoss_dW2\n",
        "W1_new = W1 - learning_rate * dLoss_dW1\n",
        "\n",
        "print(f\"Change in W1 (dLoss/dW1 * LR):\\n{(dLoss_dW1 * learning_rate).round(4)}\")\n",
        "print(f\"New W1:\\n{W1_new.round(4)}\")\n",
        "print(f\"Change in W2 (dLoss/dW2 * LR):\\n{(dLoss_dW2 * learning_rate).round(4)}\")\n",
        "print(f\"New W2:\\n{W2_new.round(4)}\")"
      ]
    }
  ]
}