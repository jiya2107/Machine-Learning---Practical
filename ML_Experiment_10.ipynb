{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWNZIEDNmULdmVQxthUSJv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiya2107/Machine-Learning---Practical/blob/main/ML_Experiment_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idTWgTVWt1br"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Bipolar inputs (X) and Bipolar targets (y) for AND gate\n",
        "# Input: (X1, X2), Target (y)\n",
        "training_data = [\n",
        "    (np.array([-1, -1]), -1),\n",
        "    (np.array([-1, 1]), -1),\n",
        "    (np.array([1, -1]), -1),\n",
        "    (np.array([1, 1]), 1)\n",
        "]\n",
        "\n",
        "# Initialize weights and bias\n",
        "num_inputs = 2\n",
        "weights = np.zeros(num_inputs)\n",
        "bias = 0\n",
        "learning_rate = 1.0 # Standard for Hebbian rule\n",
        "\n",
        "print(\"--- Hebbian Learning Implementation (Bipolar AND) ---\")\n",
        "print(f\"Initial Weights: {weights}\")\n",
        "print(f\"Initial Bias: {bias}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Training loop\n",
        "for epoch, (X, target) in enumerate(training_data):\n",
        "    print(f\"Epoch {epoch+1}: Input X={X}, Target y={target}\")\n",
        "\n",
        "    # Hebbian Weight Update Rule: W_new = W_old + LR * X * y\n",
        "    delta_W = learning_rate * X * target\n",
        "    weights = weights + delta_W\n",
        "\n",
        "    # Hebbian Bias Update Rule: Bias_new = Bias_old + LR * y\n",
        "    delta_Bias = learning_rate * target\n",
        "    bias = bias + delta_Bias\n",
        "\n",
        "    print(f\"  Delta W: {delta_W}\")\n",
        "    print(f\"  New Weights: {weights}\")\n",
        "    print(f\"  New Bias: {bias}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Test the final weights (using a simple activation function)\n",
        "def hebbian_predict(X, W, B):\n",
        "    net_input = np.dot(X, W) + B\n",
        "    return 1 if net_input >= 0 else -1\n",
        "\n",
        "print(\"\\n--- Final Prediction Test ---\")\n",
        "for X, target in training_data:\n",
        "    prediction = hebbian_predict(X, weights, bias)\n",
        "    print(f\"Input: {X}, Target: {target}, Prediction: {prediction}\")"
      ]
    }
  ]
}